---
layout: post
title:  "델타 레이크: 고성능 ACID 테이블 저장소"
date:   2020-11-11 08:00:00 +0900
categories: psyoblade created
---

# 델타 레이크: 고성능 ACID 테이블 저장소

> [Delta Lake: High-Performance ACID Table Storage over Cloud Object Store](https://www.databricks.com/research/delta-lake-high-performance-acid-table-storage-overcloud-object-stores) 논문을 읽고 요약 합니다

## 요약

>  S3와 같은 클라우드 저장소의 경우 로컬 저장소와 다르게 키-값 저장소의 구현에 따라 ACID 트랜잭션이 지원하지 않거나, 디렉토리 목록을 읽는 등의 메타데이터를 읽는 데에 많은 비용이 든다는 점 그리고 일관성 모장에 제약이 있는 등의 어려움이 있습니다.
>
>  델타 레이크는 클라우드 객체 저장소 상에서 ACID 한 테이블 저장소 레이어를 제공하면서 효과적인 메타데이터 활용 뿐만 아니라 데이터 자동적인 파일 레이아웃 최적화, 업서트, 캐싱 그리고 감사 로그 등의 다양한 기능을 제공합니다.

## 1. 소개

>  현대의 분산 저장소(HDFS, S3, GFS 등)는 대용량 데이터의 저장과 읽기에 최적화 되어 있으며, 클라우드 기반의 객체 저장소의 경우는 키-값 저장소의 특성을 가진 블록 저장소와는 다른 구조와 특성을 가집니다. 그럼에도 불구하고 엔터프라이즈 환경의 데이터 처리와 관리를 위한 데이터 웨어하우스 및 BI 업무를 충분히 잘 수행할 수 있습니다. 다만, 가능하다는 것과 기존의 관계형 데이터베이스가 가지고 있던 ACID 한 트랜잭션 및 격리 수준을 제공하지 않음으로 인해 어려움은 존재합니다
>
>  첫째. 여러개의 객체 업데이트의 원자성을 보장하지 않을 뿐만 아니라, 다수의 질의 수행 간에 격리를 지원하지 않으며, 실행 도중에 실패한 데이터에 대한 롤백 처리 또한 애플리케이션 수준에서 고려가 필요합니다
>
>  둘째. 수백만 개의 객체를 가진 아주 큰 테이블의 경우에 메타데이터를 처리하는 비용이 아주 크기 때문에 다양한 상황을 고려한 설계 및 프로그래밍이 필요합니다. 수백만개의 파케이 파일의 메타데이터를 읽거나 하이브의 메타스토어 정보를 추출하는 것 등이 이에 해당하며, 클라우드의 객체 저장소의 경우는 더욱 더 일부 데이터를 스킵하거나 최적화 하기에 애로사항이 더 많습니다.
>
>  엔터프라이즈 환경의 대용량 웨어하우스 및 데이터 ETL 처리의 경우 대용량의 누적 데이터의 처리 및 변경이 일별로 빈번하게 발생하는 패턴을 가지고 있으며, 최근 GDPR 준수를 위한 개인정보 정보를 관리하기 위해 일부 레코드 혹은 일부 컬럼에 대한 일괄적인 변경 등의 대규모의 변경 등의 과제를 가지고 있습니다.
>
>  이러한 문제점을 해소하기 위해 파케이 포맷으로 인코딩된 write-ahead 로그 방식을 통해 ACID 속성을 지원하는 델타 테이블을 설계하여 클라이언트는 다수의 객체들을 한 번에 업데이트 할 수 있습니다.

* Time travel : 특정 시점 스냅샷 질의 혹은 롤백
* UPSERT, DELETE and MERGE operations : 객체의 효율적인 고쳐쓰기
* Efficient streaming I/O : 낮은 레이턴시로 작은 객체를 테이블에 저장하면서, 추후 성능을 고려하여 트랜잭션을 지원하는 큰 객체로 병합이 가능하며, 추가되는 객체에 대한 테일링을 지원하여 메시지 버스로 활용 가능
* Caching : 델타 테이블의 테이블과 객체는 불변이므로, 클러스터 노드는 안전하게 로컬 디스크를 통한 캐시가 가능
* Data layout optimization : 테이블 객체의 크기를 자동 최적화 지원하여 특정 디멘젼에 지역성(locality)을 확보할 수 있는 Z오더에 따른 저장이 가능
* Schema evolution : 테이블 스키마가 변경되더라도 다시 쓰지 않고도 오래된 파케이 파일들을 읽기
* Audit logging : 트랜잭션 로그 기반의 감사 로깅

<img width="477" alt="delta-lake-fig-1" src="https://user-images.githubusercontent.com/604173/206891414-21c6b79a-5d19-4d09-a8db-bb9445d9f2bd.png">

>  3가지의 서로 다른 저장 시스템(a message queue, object store and data warehouse)들을 통한 파이프라인 구성 대비 스트림 및 테이블 저장소 양쪽을 지원하는 델타 레이크를 비교합니다. 델타 레이크를 통해 데이터의 중복 관리를 하지 않아도 되며, 저비용의 객체 저장소를 운영할 수 있습니다.
>
>  이러한 기능을 통해 기존 데이터 웨어하우스 뿐만 아니라 데이터 레이크의 특징을 통합한 "레이크 하우스" 패러다임을 가능하게하며, 관리 뿐만 아니라 성능 문제도 해소할 수 있습니다.

## 2. 특징 및 도전과제

>  클라우드 기반의 객체 저장소의 API 의 성능 관점에서의 특징과, 효율적인 테이블 관리의 애로 사항을 설명하고 테이블 형태의 데이터 집합에 대해 어떻게 관리하는 지에 대한 설명을 합니다

### 2-1. 객체 저장소 (Object Store)

* 객체 저장소의 개별 객체는 키에 의해 구분
* 파일 시스템 대비 디렉토리 혹은 객체의 이름 변경 비용이 크다
* S3 LIST 명령은 호출 당 최대 1,000개의 키 값들만 반환된다
* 수백만 객체의 데이터 집합의 LIST 호출은 수 분 이상 소요된다
* 객체 업데이트는 원자성을 가지지만 변경 시에는 객체 전체를 다시 써야만 한다
* 여러 디렉토리에 퍼져있는 객체들에 대한 원자성 지원은 일부 파일시스템만 지원한다

### 2-2. 일관성 (Consistency)

* 클라우드 객체 저장소는 개별 키의 결과적 일관성(Eventual Consistency)을 제공하되 다수의 키는 보장하지 않는다
* 클라이언트가 새로운 객체를 업로드에 성공 하더라도, 또 다른 클라이언트는 그 즉시 해당 객체를 LIST 하지는 못할 수 있다
* 완전한 일관성 모델은 클라우드 제공자마다 다르게 구현되지만, S3 경우 PUT 직후 GET 이 가능합니다
  * 단, GET 실패 직후에 추가된 해당 객체의 Negative Caching 에 의한 예외적인 상황도 있습니다

### 2-3. 성능에 영향을 주는 요소

* 객체 저장소의 높은 처리량을 얻기 위해서는 큰 순차적인 입출력과 병렬처리의 밸런스가 중요하다
* **읽기 동작**은 5~10ms 수준의 레이턴시로 동작하며 초 당 50~100mb 정도의 처리량을 보장한다
* 보다 높은 처리량을 보장받기 위해서는 병렬처리가 필수인데 VM 장비의 10G NIC 기준 dir 8~10개의 병렬처리가 적절하다
  * 10g bps = 10,240 bps = 10,485,760 bps = 1,310,720 bytes = 1,280 mbytes = 1.25gbytes 초 당 1G 전송
  * 초 당 100mb 전송이 가능하다는 가정 하에 최대 10개 정도 병렬로 전송을 하면 대역폭을 거의 다 쓰게 된다
* LIST 연산의 경우, 1kb 메타정보 조회 시에 10~100ms 정도 소요되므로, 많은 객체를 가진 경우 병렬처리가 아주 중요하다
  * 델타 레이크의 경우 분산 저장소에 메타데이터 파일로 저장되어 있으므로, 워커 노드에서 병렬로 수행하거나 혹은 드라이버 노드에서 멀티스레드로 수행합니다
* **쓰기 동작**은 전체 객체를 교체가 발생하며, 변경 사항이 전체 크기에 비해 작거나, 일부만 수정되어야 한다면, 객체는 가능한 작게 유지되는 것이 유리하다
* 결국 읽기가 자주 발생하는 테이블의 경우에는 I/O 및 메타데이터 조회를 줄이기 위해 파일의 수를 낮춰야 하지만, 변경이 자주 발생하는 테이블은 가능한 객체의 크기를 작게만들어 두어야 최소한의 변경만 이루어져 유리할 수 있다는 말이된다
* 객체 저장소의 테이블 저장소 사용 시의 시사점
  * 자주 액세스하는 데이터를 순차적으로 가까이 두고, 컬럼 기반 저장포맷을 사용합니다
  * 객체를 크게 만들되 너무 크지 않게 만들어, 삭제 및 수정의 비용을 줄입니다
  * LIST 작업을 피하고 가능한 사전순서 키 범위 요청을 통한 효과적인 동작을 수행합니다

### 2-4. 테이블 저장소의 특징

> 객체 저장소의 특징에 기반하여 테이블 형태의 데이터 집합은 크게 세 가지 접근 방법을 취하게 됩니다.

#### 1. 디렉토리로 구분된 파일

>  오픈소스 기반 엔진 들이 취하는 디렉토리 기반의 파티션 구분과 파케이와 같은 컬럼 포맷으로 저장하며, 계층 기반의 파티셔닝 기법을 통해서 LIST 연산의 비용을 줄일 수 있습니다

* 다수 객체에 대한 원자성을 보장하지 않습니다
  * 트랜잭션 실패 시, 일부 성공한 수정사항이 다른 클라이언트에 노출될 위험이 있습니다
* 결과적인 일관성을 보장합니다
  * 트랜잭션이 성공한 경우라도 클라이언트는 객체의 일부만 조회될 수 있습니다
* 다소 성능이 떨어질 수 있습니다
  * 파티셔닝이 잘 구성되어 있다고 하더라도 갯수가 많다면 파케이 혹은 ORC 파일의 메타데이터를 읽는 비용은 충분히 클 수 있습니다
* 자동화된 관리기능이 부족합니다
  * 객체 저장소는 파일의 읽고 쓰기에 대한 API는 제공하지만, 데이터웨어하우스 관리에 필요한 버전 관리, 감사 로그 및 표준화된 테이블 관리 도구를 제공하지 않습니다

#### 2. 커스텀 스토리지 엔진

>  스노우 플레이크 데이터 웨어하우스 엔진 클라우드 기반의 저장소 엔진의 경우 별도의 관리 기능을 제공함으로써 일관성 문제들을 해소할 수 있으며, 이러한 엔진에서 클라우드 개체 저장소는 덤 블록 장치로 취급될 수 있으며 표준 기술을 사용하여 클라우드 개체에 대한 효율적인 메타데이터 저장, 검색, 업데이트 등을 구현할 수 있습니다. 다만 비용적인 문제가 가장 큰 허들이 됩니다.
>
>  아파치 하이브의 경우에도 Hive Metastore 와 ORC 와 같은 포맷을 통해서 ACID 한 트랜잭션 처리를 지원하며, 하둡 분산 저장소 위에서 데이터 웨어하우스를 구성할 수 있습니다.

* 모든 데이터 접근은 메타데이터 서비스를 이용해야 하므로, 저장소를 직접 읽는 것 대비 병목이 될 수 있습니다
* 파케이와 같은 공개된 포맷을 사용하는 것 대비 별도의 커넥터를 통한 개발은 추가적인 엔지니어링 비용이 발생합니다
* 독점적인 메타데이터 서비스를 사용하므로써 특정 벤더 혹은 서비스 제공자에 락인될 수 있습니다

#### 3. 객체 저장소의 별도 메타데이터

>  델타 레이크의 접근 방법은 클라우드 저장소에 직접 메타데이터 뿐만 아니라 트랜잭션 로그를 저장하게 됩니다. 데이터는 파케이 포맷이며, 기존의 다양한 프레임워크와 호환성을 보장합니다. 
>
>  Apache Hudi 와 Apache IceBerg 또한 유사한 접근을 통해 동일한 기능들을 제공하지만, 델타 레이크는 Z-order 클러스터링, 캐싱 뿐만 아니라 백그라운드 최적화 등의 다양한 특징을 제공합니다

## 3. 델타 레이크 저장포맷 및 접근 프로토콜

>  델타 레이크 테이블은 클라우드 객체 저장소의 디렉토리 및 다양한 트랜잭션 연산을 저장하는 파일 시스템으로 구성됩니다. 클라이언트는 긍정적인 동시성 제어 프로토콜을 통해 데이터 구조를 업데이트 합니다. 

### 3-1. 저장포맷

<img width="477" alt="delta-lake-fig-2" src="https://user-images.githubusercontent.com/604173/206836484-86246fef-37d0-49ae-ac84-f5abe4f704a0.png">

#### 3-1-1. Data Objects

>  `mytable` 은 일자별로 파티셔닝 된 파케이 파일과, 모든 트랜잭션 로그를 저장하는 디렉토리로 구성되어 있으며, 파케이 파일은 컬럼 기반으로 다양한 압축 및 저장 포맷을 유지할 수 있습니다. 파케이 포맷을 사용할 수 있는 기존 프레임워크 들은 추가적인 도구 없이 사용이 가능합니다.
>
>  델타의 개별 데이터 객체는 생성된 유일한 GUID를 통해서 저장 관리되며, 이러한 객체들은 테이블의 개별 트랜잭션 수행에 따라 결정되는 파일들로 구성됩니다.

#### 3-1-2. Log

>  로그는 테이블 경로 아래의 `_delta_log` 라는 하위 디렉토리에 저장되며 제로 패딩 되는 증가하는 숫자 ID로 생성되는 JSON 파일로 이루어져 있습니다. 한 번의 구분된 트랜잭션에 의해 생성되는 원자적인 동작은 여러 *actions* 으로 구성되며 *checkpoints* 라고 말합니다. 여기에서 사용되는 *action* 에는 다음과 같은 종류가 있습니다.

* *Change Metadata*
  * 테이블의 첫 번째 버전은 반드시 **metaData** 액션이 필요하며 연속적인 **metaData** 액션은 이전의 정보를 덮어씁니다
  * 스키마, 파티션 정보, 저장 포맷 그리고 테이블에 대한 다양한 설정 옵션을 포함합니다
* *Add or Remove Files*
  * 데이터 추가 삭제 시에 발생하며, 클라이언트는 삭제 액션이 없었다면 모든 추가된 객체를 사용해도 됩니다
  * 데이터 객체의 레코드 추가 시에 전체 레코드 수, 컬럼 수준의 최대/최소/널 값 등의 데이터 통계도 같이 저장됩니다
  * 통계정보 또한 최신 버전이 이전 버전을 대체하게 됩니다
  * 삭제 액션은 레코드가 삭제된 타임스탬프 정보를 가지며, 그 즉시 삭제되지 않습니다
  * 이용자가 지정한 리텐션 시간 임계치가 지나고, 물리적인 삭제는 이루어집니다
  * 동시에 수행되는 다른 클라이언트의 읽기 작업도 이러한 지연된 삭제를 통해 문제없이 동작합니다
  * *dataChange flag*
    * **dataChange** 플래그는 추가 삭제 액션 시에 **false** 설정을 통해 이번 액션은 데이터를 재배치 및 통계정보에만 영향을 준다는 것을 의미합니다
    * 트랜잭션 로그를 테일링 하는 스트리밍 쿼리 수행 시에 이러한 플래그 값을 활용하여 테이블의 정렬 순서를 변경하는 등의 액션들은 스킵 할 수 있습니다.
* *Protocol Evolution*
  * 프로토콜 액션은 주어진 테이블을 읽고 쓰기 위해서 필요한 델타 프로토콜의 버전을 증가시킬때 사용됩니다
  * 클라이언트가 사용하는 기능의 호환성 보장 여부를 확인하기 위해서 사용합니다
* *Add Provenance Information*
  * 매 로그 레코드 객체는 어떤 사용자가 작업을 수행했는지를 포함하는 **commitInfo** 작업 등을 포함할 수 있습니다
* *Update Application Transaction IDs*
  * 델타 레이크는 로그 레코드 내부에 자신만의 데이터를 포함하게 할 수 있습니다
  * *exactly-once* 시맨틱을 구현하기 위해서는 이전에 마지막으로 커밋된 정보가 필요하며, 애플리케이션 실패 시에 이전 마지막 성공 이후의 작업을 이어서 수행하기 위해서 델타 레이크는 **appId** 와 **version** 필드를 가진 커스텀 **txn** 액션을 제공하여 로그 레코드 수준에서 어플리케이션 수준의 *offset* 정보를 남길 수 있습니다
  * 로그에 원자적으로 삽입되는 해당 Delta 추가 및 제거 작업과 동일한 로그 레코드에 이 정보를 배치함으로써 응용 프로그램은 Delta Lake가 새 데이터를 추가하고 해당 버전 필드를 원자적으로 저장하도록 할 수 있습니다
  * 개별 애플리케이션은 유일한 ID 인 **appId** 생성을 통해 스파크의 구조화된 스트리밍 애플리케이션 커넥터를 통해 데이터 레이크를 활용할 수 있습니다

#### 3-1-3. Log Checkpoints

>  성능 향상을 위해 반드시 로그는 주기적으로 압축하여 *checkpoints* 를 통해서 저장되어야 하며, 체크포인트는 모든 중복되지 않은 액션들을 로그 레코드 ID와 함께 저장되며, 불필요하다 판단되는 아래와 같은 액션들은 제거될 수 있습니다.
>
>  체크포인팅 프로세스의 결과로는 *add*, *remove*, *txn*, *protocol* and *changeMetadata* 등의 레코드를 가진 컬럼지향 파일로 구성되어 조회하기에 용이한 파일 포맷이며, L**IST 명령 수행시, 기존 방식의 경우 모든 파케이 파일의 footer 정보를 통해 읽어오는 것에 비해 통합된 메타데이터 파일에 의한 조회가 월등히 좋은 성능**을 보여줍니다.

* *add* 액션 직후에 동일한 데이터 객체에 대한 *remove* 액션이 따르는 경우
  * 해당 객체는 더 이상 테이블에 존재하지 않기 때문에 제거될 수 있습니다
  * 삭제 액션들은 테이블의 리텐션 설정 구성에 따라 삭제 표식으로 유지되어야 합니다
  * 특히 저장소로부터 객체를 삭제하기 위한 시점을 결정하기 위해 타임스템프를 사용합니다
* 동일한 데이터 객체의 반복된 *add* 액션은 마지막에 추가된 객체에 의해 대체될 수 있습니다
  * 새로운 객체의 경우 통계정보에만 반영될 수 있기 때문
* 동일한 *appId* 로부터 발생한 반복되는 *txn* 액션은 가장 최신 트랜잭션으로만 교체될 수 있습니다
  * 해당 트랜잭션에 최신 필드를 포함하고 있기 때문
* *protocol* 과  *changeMetadata* 액션은 최신 메타데이터로 통합되어질 수 있습니다

>  모든 클라이언트는 주어진 로그 레코드 ID까지 체크포인트 생성을 시도할 수 있으며 성공하면 해당 ID에 대해 .parquet 파일로 생성됩니다. 예를 들어 `000003.parquet`는 `000003.json`까지의 레코드 체크포인트를 나타내며, 기본 설정으로 매 **10번의 트랜잭션에 한 번씩 체크포인트**를 저장하게 됩니다.
>
>  마지막으로 Delta Lake 테이블에 액세스하는 클라이언트는 `_delta_log` 디렉터리의 모든 개체를 나열하지 않고 마지막 체크포인트(및 로그 꼬리)를 효율적으로 찾아야 합니다. 체크포인트 작성자는 새 체크포인트 ID가 해당 파일의 현재 ID보다 최신인 경우 `_delta_log/_last_checkpoint` 파일에 기록합니다. `_last_checkpoint` 파일은 클라우드 개체 저장소와의 최종 일관성 문제로 인해 오래되어도 괜찮습니다. **클라이언트는 이 파일의 ID 이후에 새 검사점을 계속 검색**하기 때문입니다.

### 3-2. 접근 프로토콜

>  델타 레이크의 접근 프로토콜은 객체 저장소의 '궁극적 일관성 보장'을 보장하면서도 직렬화 트랜잭션들을 만족하도록 설계 되었으며, 클라이언트가 테이블의 특정 버전을 읽기 위해서 알아야 할 파일이 `000003.json` 과 같은 로그 레코드 객체입니다. 주어진 데이터의 내용에 따라 클라이언트는 객체 저장소의 다른 객체들에 대해 질의할 수 있으며, '궁극적 일관성'에 의해서 보이지 않는 데이터는 기다려야 할 수도 있으며, 낙관적인 동시성 제어를 위해 다음 로그 레코드 생성은 싱글 `Writer` 만 가능하다고 간주합니다.

#### 3-2-1. 테이블 읽기

>  델타 테이블의 읽기 전용 트랜잭션에 읽기의 다섯 단계에 대해 기술합니다.

1. 만일 존재한다면, 로그 디렉토리의 `_last_checkpoint` 객체를 읽어서 최신 체크포인트 ID를 획득

2. 시작 키가 마지막 체크포인트 ID (존재하면) 이거나, 아니면 0인 `LIST` 연산을 사용하여, 로그 디렉토리의 최신 `.json` 혹은 `.parquet` 파일을 찾고, 이를 통해 가장 최신의 체크포인트로부터 시작하는 테이블의 상태를 재구성하는 데에 사용합니
   * '궁극적 일관성 보장' 특성에 의해 `LIST` 연산은 연속적인 객체들을 반환하지 못할 수 있으며, `000004.json` 그리고 `000006.json` 과 같이 `000005.json` 이 없는 결과가 나올 수도 있으나, 클라이언트는 반환된 가장 큰 ID를 사용하여 해당 버전의 테이블 정보를 조회하며, 보여질때 까지 누락된 객체들을 기다리게 됩니다
3. 만일 존재한다면, 체크 포인트를 사용하거나 이전 단계에서 식별된 후속 레코드를 사용하여 테이블의 상태를 재구성합니다
   * add 레코드는 있지만 해당 delete 레코드는 없는 데이터 개체 집합과 같은 경우, 병렬 처리가 가능합니다
4. 읽기 질의와 관련된 데이터 객체 파일 집합을 식별하기 위해서 통계 정보를 사용합니다
5. 객체 저장소에 질의하여 클러스터 전체에서 병렬로 관련 데이터 객체를 읽습니다. 클라우드 객체 저장소의 '최종 일관성'으로 인해 일부 워커 노드는 질의에 실패할 수 있습니다. 하지만 짧은 시간 후에 간단히 재시도 할 수 있습니다

>  이 단계는 분산 객체 저장소의 '**궁극적 일관성**'을 허용하도록 설계된 프로토콜입니다.
>
>  클라이언트가 `_last_checkpoint` 파일로 부터 상태 버전을 읽어왔다고 하더라도 이후에 다시 새로운 로그 파일이 발견될 수 있습니다. 즉, `_last_checkpoint` 파일은 최근 체크포인트 ID 값을 제공함으로써 `LIST` 연산의 비용을 줄이는 것으로 도움을 주는 과정이라고 말할 수 있습니다. 마찬가지로 클라이언트는 최근 레코드를 나열할 때에 불일치(로그 레코드 ID의 간격)를 허용하거나, 객체 저장소에서 아직 발견되지 않지만, 해당 데이터를 읽을 수도 있습니다.

#### 3-2-2. 쓰기 트랜잭션

> 데이터를 쓰는 트랜잭션 또한 작업의 종류에 따라 최대 5단계로 진행됩니다

1. 읽기 프로토콜의 1~2단계를 통해서 마지막 체크포인트를 통해 레코드 ID를 확인합니다. 트랜잭션은 테이블 버전 *r* 에서 데이터를 읽고 로그 레코드 *r+1* 쓰기를 시도하게 됩니다.
2. 필요한 경우 읽기 프로토콜과 동일한 단계를 통해 테이블 버전 *r* 에서 데이터를 읽습니다
3. 쓰기 트랜잭션은 데이터 디렉토리에 신규로 생성된 GUID를 통해 새 데이터 객체를 생성하며, 이 단계는 병렬로 수행될 수 있습니다. 결국 이 과정을 통해 객체들은 새로운 로그 레코드에서 참조될 준비가 완료됩니다.
4. 어떤 다른 클라이언트도 이 객체를 쓰지 않은 경우에, *r+1 .json* 로그 객체에 **로그 레코드를 저장하는 트랜잭션을 시도하게 되는데, 이 단계는 원자적이어야 하고**, 만일 이 단계에서 실패한다면 트랜잭션은 재시도될 수 있습니다.
   * 쿼리의 시맨틱에 따라 3단계에서 작성한 데이터 객체를 재활용할 수도, 새로운 레코드의 테이블에 추가할 수도 있습니다
5. 필요에 따라, 로그 레코드 *r+1* 에 대한 새로운 *.parquet* 체크포인트를 작성하고, `_last_checkpoint` 파일에 *r+1* 을 체크포인팅 하게 저장합니다
   * 기본 설정으로는 10개의 레코드 저장시 마다 체크포인팅 하게 됩니다.

#### 로그 레코드의 원자적 추가 (Adding Log Records Atomatically)

>  4번째 스텝에서 반드시 원자적이어야 하므로 반드시 하나의 클라이언트만 성공하게 됩니다. 그리고 불행하게도 모든 라지 스케일 저장소가 원자적인 `put-if-absent` 연산을 지원하지 않기 때문에 구현에 따른 우회 혹은 대안을 마련해야 합니다.

* Google Cloud Storage 혹은 Azure Bloc Store 는 `atomic put-if-absent` 지원
* HDFS 경우 완전히 동일하지 않으나, `atomic rename` 을 통해 임시 파일을 대상 이름으로 변경하는 명령으로 지원하며 Azure Data Lake Storage 경우에도 동일한 접근 방식의 `atomic rename` 을 지원합니다
* S3 경우 유사하거나 동일한 동작을 지원하지 않기 때문에, Databricks 서비스 배포판에서는 별도의 경량화된 코디네이션 서비스를 통해서 **하나의 클라이언트만 각 로그 ID로 레코드 추가가 가능**하도록 제한 합니다
  * 이 코디네이션 서비스는 쓰기에만 사용하기 때문에 부하는 낮습니다

### 3-3. 가능 한 격리 수준

>  델타 레이크의 동시성 제어 프로토콜에서 모든 쓰기 트랜잭션은 직렬화 가능의 상태이므로 레코드 ID 값이 증가하는 순서대로 순차적인 일정이 생성됩니다. 이것은 **하나의 트랜잭션만 각 레코드 ID로 레코드를 쓸 수 있**는 쓰기 트랜잭션에 대한 커밋 프로토콜을 따르게 됩니다.
>
>  읽기 트랜잭션은 스냅샷 격리 혹은 직렬화 둘 중에 하느의 격리 수준을 가지게 됩니다. 3.2.1 섹션에서는 스냅샷 읽기만 사용했고, 이러한 프로토콜을 사용하는 경우 `snapshot isolation` 격리 수준을 얻게 되며, `serializable read` 를 수행하고 싶다면 더미 쓰기를 수행하는 읽기 쓰기 트랜잭션을 수행할 수도 있습니다.
>
>  실제, 델타 레이크 커넥터 구현은 최근 로그 레코드 ID 들을 캐시 기능을 구현하고 있으며, 클라이언트는 읽기에 스냅샷 격리를 사용하더라도 "자신의 쓰기"를 읽을 수 있습니다. 그리고 다수의 읽기를 수행할 때에는 단조증가 하는 테이블 시퀀스에 따른 읽기를 수행하게 됩니다

### 3-4. 트랜잭션 레이트

>  델타 레이크의 쓰기 트랜잭션 비율은 새로운 로그 레코드를 쓰기 위한 `put-if-absent` 연산의 레이턴시에 의해 제약되며, 모든 낙관적 동시성 제어 프로토콜과 마찬가지로 쓰기 트랜잭션의 비율이 높아지면 커밋 실패가 발생하게 됩니다. 실제 **객체 저장소에 쓰기 대기시간은 수집~수백 밀리초**가 될 수 있으며, **쓰기 트랜잭션의 속도는 초당 여러개의 트랜잭션으로 제한**됩니다. 이러한 설정이 대부분의 델타 레이크 애플리케이션에서 충분히 잘 동작하는 것을 발견했습니다.

## 4. 델타의 핵심 기능

>   전통적인 분석용 데이터베이스 시스템이 제공하는 많은 기능들과 유사한 관리 기능을 제공하며, 가장 널리 사용하는 기능 위주로 소개하고, 도전 과제들에 대해 소개합니다

### 4-1. 시간여행과 롤백

* 외부 시스템으로부터 입수된 *dirty* 데이터의 경우에 기존 시스템에서는 이미 추가된 데이터의 변경을 되돌리기 어렵습니다

* 기계학습과 같은 데이터 분야의 경우는 특정 시점의 데이터를 지속적으로 사용해야 할 필요가 있습니다

* 이러한 다양한 문제들을 해결하기 위해서는 복잡한 파이프라인 오류를 수정하거나, 중복 데이터 집합을 유지해야 했습니다

* 델타 레이크의 객체와 로그는 변경할 수 없으므로 일반적인 MVCC 구현에서와 같이 데이터 과거 스냅샷은 쉽게 쿼리가 가능

* 클라이언트는 이전 로그 레코드 ID 기준으로 테이블 상태를 읽기만 하면 됩니다.

* 시간 여행을 용이하게 하기 하기위해 테이블 당 데이터 보존연한을 구성할 수 있고 SQL `AS OF` 및 `VERSION AS OF commit_id` 구문을 지원합니다.

* 일부 사용자의 데이터를 덮어쓴 업데이트를 효과적으로 취소하기 위해 아래와 같은 구문을 활용할 수 있습니다

  * ```sql
    MERGE INTO mytable target
    USING mytable TIMESTAMP AS OF <old_date> source
    ON source.userId = target.userId
    WHEN MATCHED THEN UPDATE SET *
    ```

  * 원본 source 테이블과 userId 가 매칭되는 모든 레코드에 대해 최신 테이블 target 에 업데이트 합니다

* 외에도 기존 스냅샷 중 하나에서 시작하는 테이블의 새 버전쓰기 복사를 수행하는 `CLONE` 명령도 개발되고 있습니다

### 4-2. 효율적인 UPSERT, DELETE 그리고 MERGE

* 시간이 지남에 따라 변경되어야 하는 데이터들이 존재하며, 특히 GDPR 과 같은 규정 준수를 위한 수정이 필요합니다
* 일부 필드의 수정 혹은 특정 데이터를 삭제해야 하는 발생할 수 있습니다
* 업스트림 데이터 수집 오류 혹은 지연 데이터에 의한 레코드 업데이트가 필요한 경우도 있습니다
* 집계 데이터 집합의 경우 시간이 지남에 따라 업데이트가 필수적입니다
* 델타 레이크는 모든 변경 작업을 트랜잭션 방식으로 수행하며, 표준 SQL인 UPSERT, DELETE 및 MERGE 를 지원합니다

### 4-3 스트리밍 입수 및 소비

* 많은 데이터 팀이 스트리밍 파이프라인을 ETL에 배포 및 집계하기 원하지만 클라우드 환경의 데이터 레이크는 어렵습니다
  * 종종 카프카 혹은 키네시스 같은 별도의 스트리밍 메시지 버스를 활용합니다
  * 이 과정에서 중복된 데이터와 관리적인 복잡성이 늘어나게 됩니다
* 테이블 로그가 데이터 생산자, 소비자 모두 테이블을 메시지 대기열로 처리할 수 있도록 델타 레이크를 설계했습니다
  * 크게 아래의 3가지 요소가 테이블을 통한 스트리밍 데이터 입수가 가능합니다
* **Write Compaction** 
  * 빠른 쓰기에 유리한 작은 데이터 생성과, 읽기에 최적화된 큰 데이터 모두를 만족하는 매커니즘을 제공합니다
  * 작은 객체를 단순히 쓰기만 하면 쉽게 저장은 가능하지만, 쿼리 성능에 큰 영향을 줄 수 있습니다.
  * 수많은 파일을 읽어야 하고, 그에 따른 메타데이터 조회 시간 또한 느려질 수 있습니다
    * 작은 데이터 객체를 트랜잭션 방식으로 압축하는 백그라운드 프로세스를 통해 최적화 합니다
  * 스트리밍 컨슈머가 순차적인 로그를 이미 읽은 경우, 압축된 로그 레코드들의 *dataChange false* 설정으로 회피
  * 스트리밍 애플리케이션은 오래된 데이터에 대한 쿼리가 빠르게 유지되는 동안 작은 객체를 통해 빠른 전송이 가능
    * 스트리밍 애플리케이션이 기동되는 동안에는 읽기는 수시로 읽기 때문에 작은 데이터 읽어도 문제 없음
* **Exactly-Once Streaming Writes**
  * 저장 실패시에 중복된 저장을 방지하기 위해서 멱등하게 동작하는 매커니즘을 제공합니다
  * 하나, 데이터베이스에서 취하는 접근이며, 각 레코드의 고유키가 있는지 확인하는 방법
  * 둘, 보다 일반적인 접근은 각 쓰기와 함께 "마지막으로 작성된 버전의" 레코드를 원자적으로 업데이트하여 새로운 변경 사항을 쓰는 데에만 사용할 수 있도록 하는 방법이 있습니다.
    * 결국 파일을 쓰기 위해서는 델타의 상태를 읽어 들여야 하고, 이 때에 마지막으로 저장한 정보를 확인할 수 있음
  * 델타 레이크는 애플리케이션이 각 트랜잭션과 함께 `appId, version` 쌍을 업데이트 할 수 있도록 하여 두 번째 패턴을 지원할 수 있습니다
    * *Spark Stuructured Streaming* 의 경우 커넥터에서 이러한 기능을 통해 *append, aggregation, upsert* 를 지원
* **Efficient Log Tailing**
  * 소비자가 새로운 쓰기를 효율적으로 찾을 수 있는 매커니즘을 제공합니다
  * 사전 순으로 증가하는 ID를 포함한 *.json* 로그 저장 형식, 새로운 항목은 항상 마지막 레코드 ID로 생성
  * *dataChange* 플래그를 통해 스트리밍 컨슈머는 데이터 압축, 재정렬 등의 레코드를 건너띌 수 있음
  * 스트리밍 애플리케이션 실행 시에 마지막 레코드 ID 기억만으로 델타 레이크 테이블의 동일한 레코드에서 재시작 가능

### 4-4. 데이터 레이아웃 및 최적화

>  분석 쿼리 특성상 어떤 쿼리가 수행될지 판단하기 어렵기 때문에 데이터 레이아웃은 쿼리의 성능에 아주 큰 영향을 미칩니다. 예를 들어 백그라운드 프로세스는 다른 클라이언트에 영향을 주지 않고 데이터 압축 및 레코드 순서 변경 및 통계, 인덱스 같은 보조 데이터 구조를 갱신할 수 있습니다.

#### `OPTIMIZE` 커맨드

>  사용자는 진행 중인 트랜잭션에 영향을 주지 않고 객체를 압축하는 `OPTIMIZE` 명령을 수행할 수 있고, 누락된 통계 계산을 수동으로 할 수도 있습니다. 기본적으로 데이터 객체의 크기를 1GB로 만드는 것을 목표로 하며, 이 값은 사용자가 지정할 수 있습니다.

#### 다양한 속성에 의한 `Z-Ordering`

>  많은 데이터 집합은 여러 속성과 함께 매우 선택적인 쿼리를 수신하게 되는데, 네트워크에 전송된 데이터에 대한 저장정보(*sourceIp, destIp, time*)를 튜플로 하는 데이터 집합은 여러 차원에 따라 선택적인 쿼리를 수행할 수 있습니다.
>
>  *Apache Hive* 경우 속성 값이 충분히 크지 않다면 파티셔닝 기법을 통해 해결할 수 있지만, 속성의 값의 *cardinality* 가 충분히 큰 경우에는 파티션 수가 너무 커져서 오히려 문제가 될 수 있습니다. 결국 많은 *dimension* 에 대한 높은 *locality* 를 보장하기 위해서는 주어진 속성 집합을 따라 *Z-Order* 의 순서대로 레코드를 재구성할 수 있습니다. *Z-Order* 곡선은 지정된 모든 차원에서 지역성을 유지하면서 순서를 유지하면서 생성되는 곡선입니다.
>
>  이용자가 이러한 *Z-order* 를 직접 지정하여 `OPTIMIZE` 명령 수행이 가능하며, 데이터 통계와 같이 동작하기 때문에 쿼리 수행 시에 작은 데이터를 읽어들이는 경우, 효과적인 데이터 건너뛰기가 가능합니다.

#### `AUTO OPTIMIZE` 

>  데이터 브릭스 클라우드 서비스에서는 이 속성을 통해 데이터 객체를 자동으로 압축할 수 있으며, 테이블을 업데이트 할때 인덱스 또한 계산 비용이 많이 드는 통계정보를 유지관리할 수 있습니다

### 4-5. 캐싱

>   많은 클라우드 사용자는 임시 쿼리 워크로드를 위해 상대적으로 수명이 긴 클러스터를 실행하여 워크로드에 따라 클러스터를 자동으로 확장 및 축소할 수 있습니다. 이러한 클러스터에는 객체 저장소 데이터를 로컬 장치에 캐싱하여 가속화할 수 있는데 *AWS i3 인스턴스는* *237GB 의 NVMe SSD* 저장소를 제공합니다.
>
>  데이터브릭스에서는 데이터 및 로그 객체를 캐싱하여 테이블 수준에서 데이터 및 메타데이터 쿼리를 가속화하는 클러스터의 데이터 레이크 데이터를 투명하게 캐싱하는 기능을 제공합니다. 델타 레이크 테이블의 체크포인트 데이터 객체는 *immutable* 이므로 캐싱에 안전합니다.

### 4-6. 오딧

>  데이터 레이크 트랜잭션 로그는 *commitInfo* 레코드를 기반으로 하는 감사 로깅에 사용할 수 있습니다. 데이터브릭스에서 사용자 정의함수가 클라우드 스토리지에 직접 액세스 할 수 없는 *locked-down execution mode* 를 제공합니다. 
>
>  이를 통해 런타임 엔진만 *commitInfo* 레코드를 쓸 수 있도록 하여, 임의로 해당 정보를 변경하거나 수정할 수 없어서 변경 불가능한 감사 로그를 보장합니다.

<img width="477" alt="delta-lake-fig-3" src="https://user-images.githubusercontent.com/604173/206907974-57ecf975-3d8a-4136-bf47-b4ec98e61220.png">

> Figure 3: 데이터브릭스 델타 레이크 테이블의 `DESCRIBE HISTORY` 명령을 통한 실행 이력 출력 결과.

### 4-7. 스키마

### 4-8. 외부 연동

## 5. 사례

### 5-1. 데이터 엔지니어링

### 5-2. 데이터 웨어하우스

### 5-3.

## 6. 성능

## 7. 결론



## 8. 부록

### 1. 질문과 답변

#### Q1. S3 의 Negative Caching 이 뭔가요?

> <kbd>답변</kbd> : [네거티브 캐싱 (negative caching) / 음성 캐싱 / 음수 캐싱 / 부정 응답](http://www.codns.com/b/B05-216)은 마치 블랙리스트처럼 도메인레코드가 누락되었다는 사실을 "부정적인"캐시에 그 리스트(도메인 정보 테이블)들을 저장하고, 일정시간 동안 다시 찾지 않게 하여 **불필요한 네트워크 트래픽과 DNS 검색 지연을 줄이는 역할을 하는 DNS용 캐시**를 의미합니다 - [RFC 2308](https://www.rfc-editor.org/rfc/rfc2308)

#### Q2. Optimistic Concurrency Control 이 뭔가요?

> <kbd>답변</kbd> : **[낙관적 병행 수행 제어](https://ko.wikipedia.org/wiki/%EB%82%99%EA%B4%80%EC%A0%81_%EB%B3%91%ED%96%89_%EC%88%98%ED%96%89_%EC%A0%9C%EC%96%B4)**(Optimistic concurrency control, OCC), **낙관적 동시성 제어** 또는 **낙관적인 잠금**(optimistic locking)은 [관계형 데이터베이스 관리 시스템](https://ko.wikipedia.org/wiki/관계형_데이터베이스)과 [소프트웨어 트랜잭셔널 메모리](https://ko.wikipedia.org/wiki/소프트웨어_트랜잭셔널_메모리)와 같은 트랜잭션 시스템에 적용되는 [동시성 제어](https://ko.wikipedia.org/wiki/동시성_제어) 방식이다. OCC는 복수의 트랜잭션이 서로를 간섭하지 않고도 종종 완수될 수 있다고 가정한다. 실행 중에 트랜잭션은 이러한 자원들에 락(lock)을 획득하지 않은 채 데이터 자원을 사용한다. 커밋 전에 각 트랜잭션은 다른 트랜잭션이 읽힌 데이터를 수정하지 않음을 확인한다. 이 검사 도중 충돌되는 수정이 확인되면 커밋 중인 트랜잭션은 롤백되고 다시 시작이 가능하다.
>
>  즉, 분산 저장소를 기반으로 하는 하이브 혹은 델타 테이블을 읽고 쓸 때에 누구도 같이 사용하는 클라이언트가 없다는 가정 하에 데이터를 읽고 쓸 수 있다고 가정함으로써, 높은 처리량과 낮은 복잡도를 유지하는 장점을 가질 수 있습니다

#### Q3.Eventual Consistency 가 뭔가요?

> <kbd>답변</kbd> : 

#### Q4. delta lake 에는 저장 및 commit 되었으나 offset 이 저장되지 않았다면 `exactly-once` 보장이 되는가?

> <kbd>답변</kbd> : 
>
> 애플리케이션이 재시도 시에 동일한 *appId* 와 *applicationId* 값으로 델타 레이크에 저장하면 되지 않을까?

#### Q5. delta lake 에는 저장 및 commit 되었으나, offset 저장 전에 애플리케이션이 종료되어 appId 가 변경되면?

> <kbd>답변</kbd> : 
>
> 복구를 위해서 *appId* 정보와 *applicationId* 정보를 다른 영구 저장소에 저장 및 복구될 수 있어야 하지 않을까? 그리고 스크 스트리밍 애플리케이션 기동 시에 applicationId 값을 명시적으로 변경해줄 수 있어야 하지 않을까?

#### Q6. 매 트랜잭션 마다 레코드 ID가 생성되는 경우 논문에서와 같이 자릿수가 6자리라면 100만개는 금새 차지 않을까?

> <kbd>답변</kbd> : 

#### Q7. 과연 몇 개의 레코드 당 체크포인트를 저장하는 것이 적절한 수치이며, 어떻게 조정할 수 있는가?

> <kbd>답변</kbd> : 

#### Q8. 아래의 인용문에서 `query's semantics` 에 따른 동작방식은 어떤 것들이 있는가?

> <kbd>답변</kbd> : 

```code
Attempt to write the transaction’s log record into the r + 1 .json log object, if no other client has written this object. This step needs to be atomic, and we discuss how to achieve that in various object stores shortly. If the step fails, the transaction can be retried;depending on the query’s semantics, the client can also reuse the new data objects it wrote in step 3 and simply try to add them to the table in a new log record.
```

#### Q9. MVCC (Multi-Version Concurrency Control) 란 ?

> <kbd>답변</kbd> : 
>
> [다중 버전 동시성 제어](https://mangkyu.tistory.com/53) 

#### Q10. `Z-Order` 혹은 `Z-Order curve` 가 무엇인가요?

> <kbd>답변</kbd> : 동일한 파일집합에 관련된 정보를 같이 배치하는 기술입니다. 이 *co-locality* 는 델타 레이크에서 자동으로 적용되어 *data-skipping* 알고리즘으로 활용되어 데이터 읽기에 아주 효과적이며, 데이터를 *Z-order* 순서대로 정렬하기 위해서  `ZORDER BY` 절에 해당 컬럼을 명시할 수 있습니다 
>
> ```sql
> OPTIMIZE events
> WHERE date >= current_timestamp() - INTERVAL 1 day
> ZORDER BY (eventType)
> ```
>
>  자주 쿼리에 사용되는 컬럼이 `eventType` 이고 해당 컬럼의 *cardinality* 가 아주 높은 경우에 효과적으로 사용될 수 있으며, 콤마 구분자를 활용하여 여러 컬럼을 추가할 수 있으나, *locality* 효율이 떨어질 수 있습니다. 반드시 수집된 통계가 있는 열에 대해 지정해야 하며 이는 데이터 건너뛰기에 *min, max, count* 등의 로컬 통계를 활용하기 때문입니다.
>
>  기본적으로 Databricks의 Delta Lake는 테이블 스키마에 정의된 처음 32개 열에 대한 통계를 수집합니다. [테이블 속성을](https://docs.databricks.com/delta/table-properties.html) 사용하여 이 값을 변경할 수 있습니다 `delta.dataSkippingNumIndexedCols`. 통계를 수집하기 위해 더 많은 열을 추가하면 파일을 작성할 때 더 많은 오버헤드가 추가됩니다.
>
>  긴 문자열에 대한 통계 수집은 비용이 많이 드는 작업입니다. 긴 문자열에 대한 통계 수집을 방지하려면 긴 문자열이 포함된 열을 피하도록 테이블 속성을 구성하거나 긴 문자열이 포함된 열을 사용하는 것 `delta.dataSkippingNumIndexedCols`보다 큰 열로 이동할 수 있습니다 .
>
> <img width="477" alt="z-order-curve" src="https://user-images.githubusercontent.com/604173/206906198-466a8da0-fff4-4411-9ea3-41f7cd42e16a.png">
>
>  `Z-Order curve` 란 데이터 포인트의 지역성을 유지하면서 [다차원 데이터를 1차원으로](https://en.wikipedia.org/wiki/Space-filling_curve) 매핑하는 기술을 말합니다.
>
> <img width="477" alt="z-order-curve" src="https://user-images.githubusercontent.com/604173/206906357-1e9d6a0d-3e80-4e7e-824f-3af5fac6b1bf.png">
>
> <kbd>참고</kbd> : [Z-Order 인덱스로 데이터 건너뛰기](https://docs.databricks.com/delta/data-skipping.html), [Z-Order 커브](https://en.wikipedia.org/wiki/Z-order_curve)

#### Q11. `AUTO OPTIMIZE` 기능은 데이터브릭스 클라우드 서비스에서만 사용가능한가요?

> <kbd>답변</kbd> : 

#### Q12. `SATA SSD` 와 `NVMe SSD` 의 차이점?

> <kbd>답변</kbd> : 
>
> [M.2 SSD의 2가지 유형: SATA 및 NVMe](https://www.kingston.com/kr/blog/pc-performance/two-types-m2-vs-ssd0)

#### Q13. *locked-down execution mode* 방식이 *HDFS* 상에서도 적용이 가능한가?

> <kbd>답변</kbd> : 







#### Q#. ?

> <kbd>답변</kbd> : 

